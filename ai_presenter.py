# -*- coding: utf-8 -*-
"""AI_Presenter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1weucdqiSeuLvHAnu6lQk-ELrBEg_CbD0

# Digital AI Presenter

Objective -We've developed a solution that allows every individual to create their own digital avatars effortlessly. These avatars serve as AI presenters, delivering personalized video presentations that can be used across multiple platforms.

Key Features


1. Users  can upload a high-quality image of themselves or any image they want to Digital Avatar of or take a picture through web-cam, adhering to specified rules and constraints.

2. Input any desired text of their Digital avatar to speak.

3. Preview and Download their Video Generated.

### **2. Setting up the notebook**

Mounting Google Drive to the notebook to fetch

1.   [SMC_AI.zip]() -this has the necessary files for the models as the models we are using did not have any python wrapper
"""

import os
from google.colab import drive
drive.mount('/content/drive')
root_dir = '/content/drive/MyDrive/'
os.chdir(root_dir)
!unzip 'SMC_AI.zip'
project_dir = 'SMC_AI'
os.chdir(root_dir + project_dir)

"""Downloading model weights for the models"""

# Commented out IPython magic to ensure Python compatibility.
###FOMM
# %cd /content/drive/MyDrive/SMC_AI/first_order_model
#getting model weights
!gdown --id 1DbjXD2nS3jlyCWoJu2HGcLZZjhLC9a2J

###Wav2Lip
# %cd /content/drive/MyDrive/SMC_AI/Wav2Lip
#Downloading model weights
!gdown --id 1eAtM-Ck5RMyMMZoQuoQfYZRU5vDDwBpK -O './checkpoints/wav2lip_gan.pth'
!gdown --id 1eAtM-Ck5RMyMMZoQuoQfYZRU5vDDwBpK -O './checkpoints/wav2lip.pth'
!wget "https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth" -O "./face_detection/detection/sfd/s3fd.pth"

#GFPGAN
# %cd /content/drive/MyDrive/SMC_AI/GFPGAN
!python setup.py develop
!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models

"""Installation of libraries needed:
1. We need to install [Bark](https://github.com/suno-ai/bark) for our text to speech conversion
2. [Real-ESRGAN](https://pypi.org/project/realesrgan/) is used for upsaling image resultion
3. [Kaleido](https://pypi.org/project/kaleido/) used to handle images like saving or displaying them properly
"""

!pip install git+https://github.com/suno-ai/bark.git
!pip install pydub
!pip install ffmpeg
!pip install realesrgan
!pip install kaleido
!pip install gradio
!pip install --force-reinstall -v typing-extensions==4.8.0

"""### **2. Audio Generation from text using Bark**

1. Import necessary files from bark library
2. Preload necessary models or resources
"""

from bark import SAMPLE_RATE, generate_audio, preload_models
from IPython.display import Audio

preload_models()

"""This code below takes an audio signal as a NumPy array, converts it to 16-bit PCM format, and saves it as a mono-channel WAV file with the specified sample rate. The resulting WAV file will contain the audio data in a format that is commonly used for audio storage and playback."""

import wave
import numpy as np

def save_to_wav(audio_array, filename, sample_rate):
    # Convert audio array to 16-bit PCM format
    audio_data = (audio_array * 32767).astype(np.int16)

    # Create a new wave file
    with wave.open(filename, 'wb') as wf:
        # Set the parameters for the wave file
        wf.setnchannels(1)  # mono
        wf.setsampwidth(2)  # 16-bit PCM
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data.tobytes())

"""### **3. Run the Main Script by passing the input image and audio file**

Since the model pipeline works best with StyleGAN-like images, we need to crop the image according to certain dimensions for the model to work properly. We use [FFHQ](https://github.com/NVlabs/ffhq-dataset) method to do so since it helps to align and crop images similar to of those produced by StyleGAN.

Code flow for the run function  :

* Initialize: In this step, the necessary libraries and models are imported, and any required parameters or configurations are set up.
<br>
* FOMM (First Order Motion Model): FOMM is a model that can transfer the facial movements from one video to another. In this step, FOMM is used to extract the facial landmarks and motion information from the source video.
<br>
* Wav2Lip: Wav2Lip is a lip-syncing model that can generate realistic lip movements based on an input audio. Here, Wav2Lip is used to synchronize the lip movements with the audio of the source video.
<br>
* Vid2Frames: This step involves converting the source video into individual frames. The video is split into a sequence of frames that can be processed individually.
<br>
* GFPGAN (Round 1): GFPGAN is a model used for face image generation and enhancement. In this step, GFPGAN is applied to the frames of the source video to enhance the facial details and generate high-quality face images.
<br>
* Frames2Vid: The frames processed by GFPGAN are combined back into a video sequence.
<br>
* FOMM (Second Pass): Another pass of FOMM is performed to transfer the facial movements and motion information from the original source video to the enhanced video frames obtained from GFPGAN.
<br>
* Vid2Frames: Similar to step 4, the enhanced video is split into individual frames.
<br>
* GFPGAN (Round 2): GFPGAN is applied again to the frames obtained in step 8, further improving the facial details and generating higher-quality face images.
<br>
* Frames2Vid: The frames processed by GFPGAN in the second round are combined back into a video sequence.
<br>

* Copy to final output: The final generated video is copied to the desired output location or displayed for further processing or analysis.
<br>
* Copy to save path: The generated video is saved to a specified path for future reference or use.
<br>
* Complete: This step indicates that the process is finished successfully.
<br>
* End: This is the end of the code execution.

---



---
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/SMC_AI
# from runLIHQ import run

from runLIHQ import run

# Commented out IPython magic to ensure Python compatibility.

# %cd /content/drive/MyDrive/SMC_AI
from runLIHQ import run
# %cd /content/drive/MyDrive/SMC_AI/procedures/face_align
from face_crop import crop_face
from pydub import AudioSegment

def generate(text_prompt,language,gender,image):
  # Generate audio from the text prompt
  speaker_id = {
        'English': {'male': 'v2/en_speaker_0', 'female': 'v2/en_speaker_9'},
        'Chinese': {'male': 'v2/zh_speaker_0', 'female': 'v2/zh_speaker_4'},
        'French': {'male': 'v2/fr_speaker_0', 'female': 'v2/fr_speaker_1'},
        # Add other languages here...
    }
  s_id = speaker_id[language][gender]
  print(s_id)
  audio_array = generate_audio(text_prompt,history_prompt=s_id)

  # Play the generated audio using the IPython Audio module
  Audio(audio_array, rate=SAMPLE_RATE)

  # Save the generated audio as a WAV file
  save_to_wav(audio_array, "/content/drive/MyDrive/SMC_AI/input/audio/Folder1/1.wav", SAMPLE_RATE)
  # Set filepath of your image to be cropped, and where you want the cropped image saved.
  os.chdir('/content/drive/MyDrive/SMC_AI/procedures/face_align')
  crop_face(filename = image,
            outfile = '/content/drive/MyDrive/SMC_AI/input/face/Crop.png')

  os.chdir('/content/drive/MyDrive/SMC_AI')
  run(face='/content/drive/MyDrive/SMC_AI/input/face/Crop.png')
  video = '/content/drive/MyDrive/SMC_AI/output/finalVidsOut/Folder1.mp4'
  print(video)
  return video

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/SMC_AI
from runLIHQ import run
# %cd /content/drive/MyDrive/SMC_AI/procedures/face_align
from face_crop import crop_face
from pydub import AudioSegment

"""Using Gradio to make the web interface for our tool"""

import gradio as gr
demo = gr.Interface(
    generate,
    [
        gr.Textbox(placeholder="Type your script in the language you want here..."),
        gr.Dropdown(choices=['English', 'Chinese', 'French']),  # Add other languages here
        gr.Radio(["male", "female"]),
        gr.Image(type='filepath'),
    ],
    outputs=[gr.Video()],
    title="AI Presenter",
    description="For precise AI representation, please ensure your portrait is front-facing, and your mouth is closed during the capture process.",
    examples=[
        [
            'Hi! I am an AI Presenter ',
            "English",
            "female",
            os.path.join("/content/drive/MyDrive/SMC_AI", "input/face/examples/123456.png"),

        ],
        ],
        #os.path.join("/content/drive/MyDrive/SMC_AI", "input/face/examples/1428119.jpg"),
        #os.path.join("/content/drive/MyDrive/SMC_AI", "input/face/examples/l975279.png"),
        #os.path.join("/content/drive/MyDrive/SMC_AI", "input/face/examples/2237618.jpg"),
)
demo.launch(share=True,debug=True)

"""### **4. Experiements and Comparison**

Here we will compare three different model pipeline that we experimented with to get the best ouptut:
1. FOMM -> Wav2LIP
2. FOMM -> Wav2LIP ->GFPGAN
3. FOMM -> Wav2LIP ->GFPGAN -> FOMM - > GFPGAN

First we just gave a fixed text and voice to keep the variables same as we just want to campare the output video. So we save the generated output and use it as input for the different pipelines. We also use the same input image.
"""

# Define the text to be converted into audio
text_prompt = """
     Hi there! This is Sound and Music Computing.
"""
# Generate audio from the text prompt
audio_array = generate_audio(text_prompt,history_prompt="v2/en_speaker_9")

# Play the generated audio using the IPython Audio module
Audio(audio_array, rate=SAMPLE_RATE)

# Save the generated audio as a WAV file
save_to_wav(audio_array, "/content/drive/MyDrive/SMC_AI/input/audio/Folder1/1.wav", SAMPLE_RATE)

"""[Moviepy](https://pypi.org/project/moviepy/) is a built in library in Google Colab for video reading and writing"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/SMC_AI
import moviepy.editor

"""This is the first pipeline that we designed, FOMM -> Wav2LIP. The video was generated very quick. However, the output is blurred

"""

from runno_GFPGAN import run
run(face='/content/drive/MyDrive/SMC_AI/input/face/examples/123456.png', save_path='/content/drive/MyDrive/SMC_AI/output/finalVidsOut/without_GFPGAN.mp4')

moviepy.editor.ipython_display("/content/drive/MyDrive/SMC_AI/output/finalVidsOut/without_GFPGAN.mp4Folder1.mp4")

"""This is the second pipeline that we designed, FOMM -> Wav2LIP -> GFPGAN. The video was generated a little slower than previous one but still quick. It is visible that the output quality is improved from the last pipeline, but it is still not a very good resolution

"""

from runno_GFPGAN2 import run
run(face='/content/drive/MyDrive/SMC_AI/input/face/examples/123456.png', save_path='/content/drive/MyDrive/SMC_AI/output/finalVidsOut/without_GFPGAN2.mp4')

moviepy.editor.ipython_display("/content/drive/MyDrive/SMC_AI/output/finalVidsOut/without_GFPGAN2.mp4Folder1.mp4")

"""This is the last pipeline that we designed, FOMM -> Wav2LIPFOMM -> Wav2LIP ->GFPGAN -> FOMM - > GFPGAN. We realised that we though this takes a longer time to run, the results are high-quality pls see the video result on it through our web app. So this is our final model that we used for our web app.

"""

from runLIHQ import run
run(face='/content/drive/MyDrive/SMC_AI/input/face/examples/123456.png', save_path='/content/drive/MyDrive/SMC_AI/output/finalVidsOut/with_GFPGAN2.mp4')